{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sources : 51\n",
      "sources : abc-news-au, al-jazeera-english, ars-technica, associated-press, bbc-news, bbc-sport, bloomberg, breitbart-news, business-insider, business-insider-uk, buzzfeed, cnn, engadget, entertainment-weekly, espn, espn-cric-info, financial-times, football-italia, fortune, four-four-two, fox-sports, google-news, hacker-news, ign, independent, mashable, mtv-news, mtv-news-uk, national-geographic, new-scientist, newsweek, new-york-magazine, nfl-news, polygon, recode, reddit-r-all, reuters, talksport, techcrunch, techradar, the-hindu, the-huffington-post, the-lad-bible, the-next-web, the-sport-bible, the-times-of-india, the-verge, the-wall-street-journal, the-washington-post, time, usa-today\n",
      "category of reuters: general\n",
      "category of techcrunch: technology\n",
      "categories: ['business', 'science', 'general', 'sports', 'technology', 'entertainment']\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "def getSources():\n",
    "    source_url = 'https://newsapi.org/v1/sources?language=en'\n",
    "    response = requests.get(source_url).json()\n",
    "    sources = []\n",
    "    for source in response['sources']:\n",
    "        sources.append(source['id'])\n",
    "    return sources\n",
    "\n",
    "sources = getSources()\n",
    "print('number of sources :', len(sources))\n",
    "print('sources :', ', '.join(sources))\n",
    "\n",
    "# number of sources : 60\n",
    "# sources : abc-news-au, al-jazeera-english, ars-technica, associated-press, bbc-news, bbc-sport, bloomberg, breitbart-news, business-insider, business-insider-uk, buzzfeed, cnbc, cnn, daily-mail, engadget, entertainment-weekly, espn, espn-cric-info, financial-times, football-italia, fortune, four-four-two, fox-sports, google-news, hacker-news, ign, independent, mashable, metro, mirror, mtv-news, mtv-news-uk, national-geographic, new-scientist, newsweek, new-york-magazine, nfl-news, polygon, recode, reddit-r-all, reuters, talksport, techcrunch, techradar, the-economist, the-guardian-au, the-guardian-uk, the-hindu, the-huffington-post, the-lad-bible, the-new-york-times, the-next-web, the-sport-bible, the-telegraph, the-times-of-india, the-verge, the-wall-street-journal, the-washington-post, time, usa-today\n",
    "def mapping():\n",
    "    d = {}\n",
    "    response = requests.get('https://newsapi.org/v1/sources?language=en')\n",
    "    response = response.json()\n",
    "    for s in response['sources']:\n",
    "        d[s['id']] = s['category']\n",
    "    return d\n",
    "m= mapping()\n",
    "print('category of reuters:', m['reuters'])\n",
    "print('category of techcrunch:', m['techcrunch'])\n",
    "\n",
    "# category of reuters: general\n",
    "# category of techcrunch: technology\n",
    "print('categories:', list(set(m.values())))\n",
    "# categories: [u'business', u'entertainment', u'science', u'general', u'sports', u'technology']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "552b39688e024ddebc2cb638263e0b10",
       "version_major": 2,
       "version_minor": 0
      },
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "def getSources():\n",
    "    source_url = 'https://newsapi.org/v1/sources?language=en'\n",
    "    response = requests.get(source_url).json()\n",
    "    sources = []\n",
    "    for source in response['sources']:\n",
    "        sources.append(source['id'])\n",
    "    return sources\n",
    "\n",
    "def mapping():\n",
    "    d = {}\n",
    "    response = requests.get('https://newsapi.org/v1/sources?language=en')\n",
    "    response = response.json()\n",
    "    for s in response['sources']:\n",
    "        d[s['id']] = s['category']\n",
    "    return d\n",
    "\n",
    "def category(source, m):\n",
    "    try:\n",
    "        return m[source]\n",
    "    except:\n",
    "        return 'NC'\n",
    "\n",
    "def getDailyNews():\n",
    "    sources = getSources()\n",
    "    key = 'c12a5c07a7bd42edbf54d59aca007a54'\n",
    "    url = 'https://newsapi.org/v1/articles?source={0}&sortBy={1}&apiKey={2}'\n",
    "    responses = []\n",
    "    for i, source in tqdm_notebook(enumerate(sources), total=len(sources)):\n",
    "        \n",
    "        try:\n",
    "            u = url.format(source, 'top', key)\n",
    "        except:\n",
    "            u = url.format(source, 'latest', key)\n",
    "        \n",
    "        response = requests.get(u)\n",
    "        r = response.json()\n",
    "        try:\n",
    "            for article in r['articles']:\n",
    "                article['source'] = source\n",
    "            responses.append(r)\n",
    "        except:\n",
    "            print('Rate limit exceeded ... please wait and retry in 6 hours')\n",
    "            return None\n",
    "                \n",
    "    articles = list(map(lambda r: r['articles'], responses))\n",
    "    articles = list(reduce(lambda x,y: x+y, articles))\n",
    "    \n",
    "    news = pd.DataFrame(articles)\n",
    "    news = news.dropna()\n",
    "    news = news.drop_duplicates()\n",
    "    news.reset_index(inplace=True, drop=True)\n",
    "    d = mapping()\n",
    "    news['category'] = news['source'].map(lambda s: category(s, d))\n",
    "    news['scraping_date'] = datetime.now()\n",
    "\n",
    "    try:\n",
    "        aux = pd.read_csv('./data/news.csv')\n",
    "        aux = aux.append(news)\n",
    "        aux = aux.drop_duplicates('url')\n",
    "        aux.reset_index(inplace=True, drop=True)\n",
    "        aux.to_csv('./data/news.csv', encoding='utf-8', index=False)\n",
    "    except:\n",
    "        news.to_csv('./data/news.csv', index=False, encoding='utf-8')\n",
    "        \n",
    "    print('Done')\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    getDailyNews()"
   ]
  },
  
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_columns = 200\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "from string import punctuation\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "tqdm_notebook().pandas()\n",
    "data = pd.read_csv('./data/news.csv')\n",
    "print(data.shape)\n",
    "data.head(3)"
   ]
  },
  
   "source": [
    "stop_words = []\n",
    "\n",
    "f = open('./data/stopwords.txt', 'r')\n",
    "for l in f.readlines():\n",
    "    stop_words.append(l.replace('\\n', ''))\n",
    "    \n",
    "additional_stop_words = ['t', 'will']\n",
    "stop_words += additional_stop_words\n",
    "\n",
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _removeNonAscii(s): \n",
    "    return \"\".join(i for i in s if ord(i)<128)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = text.replace('(ap)', '')\n",
    "    text = re.sub(r\"\\'s\", \" is \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r\"\\\\\", \"\", text)\n",
    "    text = re.sub(r\"\\'\", \"\", text)    \n",
    "    text = re.sub(r\"\\\"\", \"\", text)\n",
    "    text = re.sub('[^a-zA-Z ?!]+', '', text)\n",
    "    text = _removeNonAscii(text)\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    text = clean_text(text)    \n",
    "    tokens = [word_tokenize(sent) for sent in sent_tokenize(text)]\n",
    "    tokens = list(reduce(lambda x,y: x+y, tokens))\n",
    "    tokens = list(filter(lambda token: token not in (stop_words + list(punctuation)) , tokens))\n",
    "    return tokens\n",
    "data['description'] = data['description'].map(lambda d: unicode(d.decode('utf-8')))\n",
    "data['tokens'] = data['description'].progress_map(lambda d: tokenizer(d))\n",
    "for descripition, tokens in zip(data['description'].head(5), data['tokens'].head(5)):\n",
    "    print('description:', descripition)\n",
    "    print('tokens:', tokens)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keywords(category):\n",
    "    tokens = data[data['category'] == category]['tokens']\n",
    "    alltokens = []\n",
    "    for token_list in tokens:\n",
    "        alltokens += token_list\n",
    "    counter = Counter(alltokens)\n",
    "    return counter.most_common(10)\n",
    "for category in set(data['category']):\n",
    "    print('category :', category)\n",
    "    print('top 10 keywords:', keywords(category))\n",
    "    print('---')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
